<tool id='NA' name='NA'>
  <command interpreter='bash'>main.sh</command>
  <cancel interpreter='bash'>kill.sh</cancel>
  <inputs>
    <param 
      name='novnc_dir' 
      label='Default noVNC installation directory' 
      type='hidden' 
      value='__WORKDIR__/pw/bootstrap/noVNC-1.3.0'  
    ></param>
    <param 
      name='novnc_tgz' 
      label='Path to noVNC TGZ' 
      type='hidden' 
      value='/swift-pw-bin/apps/noVNC-1.3.0.tgz'  
    ></param>
    <section name='service' type='section' title='Service Inputs' expanded='false'>
      <param 
        name='name' 
        label='Service' 
        type='hidden' 
        value='turbovnc'  
      ></param>
      <param 
        name='load_env' 
        label='Command to load ecFlow' 
        type='text' 
        value='module load ecflow' 
        help='To load the environment, enter the appropriate command, for example: module load module-name or source path/to/env.sh.'  
      ></param>
      <param 
        name='bin' 
        label='Launch command' 
        type='text' 
        value='ecflow_ui' 
        help=''  
      ></param>
      <param 
        name='vnc_exec' 
        label='Path to vncserver' 
        type='hidden' 
        value='/opt/TurboVNC/bin/vncserver'  
      ></param>
      <param 
        name='background' 
        label='Run service in background (True or False)' 
        type='hidden' 
        value='False'  
      ></param>
    </section>
    <section name='pwrl_host' type='section' title='ecFlow host' expanded='true'>
      <param 
        name='resource' 
        type='computeResource' 
        label='Service host' 
        hideUserWorkspace='true' 
        hideDisconnectedResources='false' 
        help='Resource to host the service'
	      hideDisconnectedResources='false'
      ></param>
      <param 
        name='nports' 
        label='Number of Ports to Reserve' 
        type='hidden' 
        value='1'  
      ></param> 
      <param 
        name='jobschedulertype' 
        type='select' 
        label='Select Controller or SLURM Partition' 
        help='Job will submitted using SSH or sbatch, respectively'      
        multiple='false'>
          <option value="CONTROLLER" selected="true">Controller</option>
          <option value="SLURM">SLURM Partition</option>
      </param>
      <param 
        name='_sch__dd_account_e_' 
        label='SLURM account' 
        type='dynamicAccountDropdown' 
        help='Account to submit the interactive job' 
        resource='pwrl_host.resource'
        depends_on='pwrl_host.jobschedulertype'
        show_if='SLURM'
      ></param>
      <param 
        name='_sch__dd_partition_e_' 
        label='SLURM partition' 
        type='dynamicPartitionDropdown' 
        resource='pwrl_host.resource'
        account='pwrl_host._sch__dd_account_e_'
        help='Partition to submit the interactive job. Leave empty to let SLURM pick the optimal option.' 
        depends_on='pwrl_host.jobschedulertype'
        show_if='SLURM'
        optional='true'
      ></param>
      <param 
        name='scheduler_directives' 
        label='Scheduler directives' 
        type='text' 
        help='e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ; to separate parameters. Do not include the SBATCH keyword.' 
        value=''
        depends_on='pwrl_host.jobschedulertype'
        show_if='SLURM'
        optional='true'  
       ></param>
    </section>
  </inputs>
  <outputs>
  </outputs>
</tool>
