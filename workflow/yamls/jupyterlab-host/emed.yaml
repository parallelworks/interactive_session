permissions:
  - '*'
sessions:
  session:
    useTLS: false
    redirect: true


jobs:
  preprocessing:
    steps:
      - name: Preprocessing
        run: ./utils/steps-v3/preprocessing/preprocessing.sh 
      - name: Validating Target Resource
        run: ./utils/steps-v3/preprocessing/input_form_resource_wrapper.sh ${{ inputs.pwrl_host.resource.ip }}
      - name: Process Inputs
        run: | 
          ./utils/steps-v3/preprocessing/process_inputs_sh.sh 
          echo "export basepath=/me/session/${PW_USER}/${{ sessions.session }}"  >> resources/host/inputs.sh
      - name: Transfer Files to Controller
        run: ./utils/steps-v3/preprocessing/transfer_files_to_controller.sh
      - name: Controller Preprocessing
        run: ./utils/steps-v3/preprocessing/controller_preprocessing.sh
      - name: Initialize Cancel Script
        run: ./utils/steps-v3/preprocessing/initialize_cancel_script.sh

  controller_job:
    needs:
       - preprocessing
    steps:
      - name: Create Controller Session Script
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/create_session_script.sh
      - name: Launch and Monitor Controller Job
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" == "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh

  compute_job:
    needs:
       - preprocessing
    steps:
      - name: Create Compute Session Script
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/create_session_script.sh
      - name:  Launch and Monitor Compute Job
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" != "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh


  create_session:
    needs:
       - preprocessing
    steps:
      - name: Set Session Name
        run: |
          session_name=$(pwd | rev | cut -d'/' -f1-2 | tr '/' '-' | rev)
          echo "session_name=${session_name}" | tee -a $OUTPUTS
      - name: Get Controller Hostname
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          target_hostname=$(${sshcmd} hostname)
          echo "target_hostname=${target_hostname}"  | tee -a $OUTPUTS
      - name: Get Compute Hostname
        if: ${{ 'SLURM' === inputs.pwrl_host.jobschedulertype }}
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          while true; do
            echo "Waiting for target hostname..."

            # Check if the service.port file exists and read its contents
            target_hostname=$(${sshcmd} "if [ -f ~/${PWD/#$HOME/}/target.hostname ]; then cat ~/${PWD/#$HOME/}/target.hostname; fi")

            # Exit the loop if file was found and read
            if [ -n "${target_hostname}" ]; then
              echo "Target's hostname found: ${target_hostname}"
              break
            fi

            # Wait before the next check
            sleep 5
          done
          target_hostname=$(${sshcmd} squeue -j "${job_id}" --noheader --format="%N")
          echo "target_hostname=${target_hostname}"  | tee -a $OUTPUTS

      - name: Get Remote Port
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          while true; do
            echo "Waiting for service port file..."

            # Check if the service.port file exists and read its contents
            remote_port=$(${sshcmd} "bash -c \"if [ -f ~/${PWD/#$HOME/}/service.port ]; then cat ~/${PWD/#$HOME/}/service.port; fi\"")

            # Exit the loop if remote_port is successfully set (file was found and read)
            if [ -n "$remote_port" ]; then
              echo "Service port found: $remote_port"
              break
            fi

            # Wait before the next check
            sleep 5
          done
          echo "remote_port=${remote_port}"  | tee -a $OUTPUTS
      - name: Select Local Port
        run: |
          local_port=$(pw agent open-port)
          echo "local_port=${local_port}"  | tee -a $OUTPUTS
      - name: Set URL SLUG
        run: |
          echo "slug=lab"  | tee -a $OUTPUTS
      - name: Expose Port
        uses: parallelworks/update-session
        with:
          remotePort: "${{ needs.create_session.outputs.remote_port }}"
          localPort: "${{ needs.create_session.outputs.local_port }}"
          remoteHost: "${{ needs.create_session.outputs.target_hostname }}"
          status: running
          slug: "${{ needs.create_session.outputs.slug }}"
          target: ${{ inputs.pwrl_host.resource.id }}
          name: ${{ sessions.session }}
        
'on':
  execute:
    inputs:
      pwrl_host:
        type: group
        label: Jupyter Server Host
        items:
          resource:
            type: compute-clusters
            label: Service host
            include-workspace: false
            tooltip: Resource to host the service
          nports:
            type: number
            label: Number of Ports to Reserve
            hidden: true
            default: 1
            optional: true
          jobschedulertype:
            type: string
            label: Select Controller, SLURM Partition or PBS Queue
            default: SLURM
            hidden: true
          _sch__dd_partition_e_:
            type: slurm-partitions
            label: SLURM partition
            optional: true
            tooltip: Partition to submit the interactive job. Leave empty to let SLURM pick the optimal option.
            resource: ${{ inputs.pwrl_host.resource }}
          _sch__dd_cpus_d_per_d_task_e_:
            type:  number
            label: CPUs per task
            min: 1
            max: 32
            default: 1
            tooltip: --cpus-per-task=value slurm directive
          _sch__dd_mem_d_per_d_cpu_e_:
            type:  string
            label: Minimum memory required per usable allocated CPU
            default: 8GB
            tooltip: --mem-per-cpu=value slurm directive
            optional: true
          _sch__dd_time_e_:
            type:  string
            label: Walltime
            default: 01:00:00
            tooltip: e.g. 01:00:00 - Amount of time slurm will honor the interactive session.
            optional: true
          scheduler_directives:
            type: string
            label: Scheduler directives
            optional: true
            tooltip: e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ; to separate parameters. Do not include the SBATCH keyword.
        collapsed: false
      service:
        type: group
        label: Jupyter Lab Settings
        items:
          name: 
            type: string
            hidden: true
            default: jupyterlab-host
          nginx_sif:
            type: string
            hidden: true
            default: /public/apps/pw/nginx-unprivileged.sif
          password:
            label: Password for notebook session
            type: string
            optional: true
            hidden: true
            ignore: true
            tooltip: Enter password or leave blank for no password
          notebook_dir:
            label: Directory to start Jupyter Lab GUI
            type: string
            default: /gs/gsfs0/users/__USER__/pw/
            tooltip: This is the directory that you start with when the Jupyter graphical user interface starts. The default value here is your home directory.
          load_env:
            label: Command to load Jupyter Lab to the PATH
            type: string
            default: source /gs/gsfs0/hpc01/rhel8/apps/conda3/etc/profile.d/conda.sh; conda activate base; module load cuda
            tooltip: Use a bash command
