permissions:
  - '*'
sessions:
  session:
    useTLS: false
    redirect: true

jobs:
  preprocessing:
    steps:
      - name: Validating Target Resource
        early-cancel: any-job-failed
        run: ./utils/steps-v3/preprocessing/input_form_resource_wrapper.sh ${{ inputs.pwrl_host.resource.ip }}
      - name: Process Inputs
        early-cancel: any-job-failed
        run: |
          set -e
          echo "export basepath=/me/session/${PW_USER}/${{ sessions.session }}"  >> resources/host/inputs.sh
          ./utils/steps-v3/preprocessing/process_inputs_sh.sh
      - name: Transfer Files to Controller
        early-cancel: any-job-failed
        run: ./utils/steps-v3/preprocessing/transfer_files_to_controller.sh
      - name: Controller Preprocessing
        early-cancel: any-job-failed
        run: ./utils/steps-v3/preprocessing/controller_preprocessing.sh
      - name: Initialize Cancel Script
        early-cancel: any-job-failed
        run: ./utils/steps-v3/preprocessing/initialize_cancel_script.sh

  controller_job:
    needs:
      - preprocessing
    steps:
      - name: Create Controller Session Script
        early-cancel: any-job-failed
        if: ${{ 'CONTROLLER' == inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/create_session_script.sh
      - name: Launch and Monitor Controller Job
        if: ${{ 'CONTROLLER' == inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" == "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        early-cancel: any-job-failed
        if: ${{ 'CONTROLLER' == inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh

  compute_job:
    needs:
      - preprocessing
    steps:
      - name: Create Compute Session Script
        early-cancel: any-job-failed
        if: ${{ 'CONTROLLER' != inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/create_session_script.sh
      - name: Launch and Monitor Compute Job
        if: ${{ 'CONTROLLER' != inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" != "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        early-cancel: any-job-failed
        if: ${{ 'CONTROLLER' != inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh

  create_session:
    needs:
      - preprocessing
    steps:
      - name: Set Session Name
        early-cancel: any-job-failed
        run: |
          session_name=$(pwd | rev | cut -d'/' -f1-2 | tr '/' '-' | rev)
          echo "session_name=${session_name}" | tee -a $OUTPUTS
      - name: Get Controller Hostname
        early-cancel: any-job-failed
        if: ${{ 'CONTROLLER' == inputs.pwrl_host.jobschedulertype }}
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          target_hostname=$(${sshcmd} hostname)
          echo "target_hostname=${target_hostname}"  | tee -a $OUTPUTS

      - name: Get Compute Hostname
        early-cancel: any-job-failed
        if: ${{ 'SLURM' == inputs.pwrl_host.jobschedulertype || 'PBS' == inputs.pwrl_host.jobschedulertype }}
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          source utils/load-env.sh
          source resources/host/inputs.sh
          while true; do
            failIfError
            echo "Waiting for target hostname..."

            # Check if the service.port file exists and read its contents
            target_hostname=$(${sshcmd} "if [ -f ${resource_jobdir}/target.hostname ]; then cat ${resource_jobdir}/target.hostname; fi")

            # Exit the loop if file was found and read
            if [ -n "${target_hostname}" ]; then
              echo "Target's hostname found: ${target_hostname}"
              break
            fi

            # Wait before the next check
            sleep 5
          done
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" == "SLURM" ]]; then
            job_id=$(${sshcmd} cat ${resource_jobdir}/job.id)
            if [ -z "${job_id}" ]; then
              echo "Error: SLURM job ID is empty!" >&2
              exit 1
            fi
            target_hostname=$(${sshcmd} squeue -j "${job_id}" --noheader --format="%N")
          fi
          echo "target_hostname=${target_hostname}"  | tee -a $OUTPUTS

      - name: Get Remote Port
        early-cancel: any-job-failed
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          source resources/host/inputs.sh
          while true; do
            echo "Waiting for service port file..."

            # Check if the service.port file exists and read its contents
            remote_port=$(${sshcmd} "bash -c \"if [ -f ${resource_jobdir}/service.port ]; then cat ${resource_jobdir}/service.port; fi\"")

            # Exit the loop if remote_port is successfully set (file was found and read)
            if [ -n "$remote_port" ]; then
              echo "Service port found: $remote_port"
              break
            fi

            # Wait before the next check
            sleep 5
          done
          echo "remote_port=${remote_port}"  | tee -a $OUTPUTS
      - name: Waiting for Server to Start
        early-cancel: any-job-failed
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
          remote_port: ${{ needs.create_session.outputs.remote_port }}
          remote_host: ${{ needs.create_session.outputs.target_hostname }}
        run: |
          TIMEOUT=5
          RETRY_INTERVAL=3

          # Function to check if server is listening
          check_server() {
              ${sshcmd} "curl --silent --connect-timeout \"$TIMEOUT\" \"http://${remote_host}:${remote_port}\"" >/dev/null 2>&1
              return $?
          }

          # Main loop
          attempt=1
          while true; do
              echo "Attempt $attempt: Checking if server is listening on ${remote_host}:${remote_port}..."
              
              if check_server; then
                  # Sleep to wait for jupyterlab after nginx connects
                  sleep 20
                  echo "Success: Server is listening on ${remote_host}:${remote_port}!"
                  exit 0
              else
                  echo "Server not responding. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
              fi
          done
      - name: Select Local Port
        early-cancel: any-job-failed
        run: |
          local_port=$(pw agent open-port)
          echo "local_port=${local_port}"  | tee -a $OUTPUTS
      - name: Set URL SLUG
        early-cancel: any-job-failed
        run: |
          echo "slug=lab"  | tee -a $OUTPUTS
      - name: Expose Port
        early-cancel: any-job-failed
        uses: parallelworks/update-session
        with:
          remotePort: '${{ needs.create_session.outputs.remote_port }}'
          localPort: '${{ needs.create_session.outputs.local_port }}'
          remoteHost: '${{ needs.create_session.outputs.target_hostname }}'
          slug: '${{ needs.create_session.outputs.slug }}'
          target: ${{ inputs.pwrl_host.resource.id }}
          name: ${{ sessions.session }}

'on':
  execute:
    inputs:
      pwrl_host:
        type: group
        label: Service Host
        items:
          resource:
            type: compute-clusters
            label: Service host
            include-workspace: false
            tooltip: Resource to host the service
          nports:
            type: number
            label: Number of Ports to Reserve
            hidden: true
            default: 1
          jobschedulertype:
            type: dropdown
            label: Select Controller or SLURM Partition
            default: CONTROLLER
            options:
              - value: CONTROLLER
                label: Controller
              - value: SLURM
                label: SLURM Partition
              - value: PBS
                label: PBS Queue
            tooltip: Job will be submitted using SSH, sbatch or qsub, respectively
          _sch__dd_account_e__tag_existing:
            label: SLURM account
            type: string
            tooltip: Account to submit the interactive job
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' != inputs.pwrl_host.resource.provider }}
            ignore: ${{ .hidden }}
            optional: ${{ .hidden }}
          _sch__dd_partition_e__tag_existing:
            label: SLURM partition
            type: slurm-partitions
            resource: ${{ inputs.pwrl_host.resource }}
            tooltip: SLURM partition to submit the interactive job
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' != inputs.pwrl_host.resource.provider }}
            ignore: ${{ .hidden }}
            optional: true
          qos_tag_existing:
            label: Quality of Service [QoS]
            type: slurm-qos
            resource: ${{ inputs.pwrl_host.resource }}
            tooltip: Select a QOS from the drop down menu
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' != inputs.pwrl_host.resource.provider }}
            ignore: ${{ .hidden }}
            optional: ${{ .hidden }}
          _sch__dd_ntasks_e__tag_existing:
            label: Number of tasks
            type: number
            min: 1
            max: 100
            default: 1
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' != inputs.pwrl_host.resource.provider }}
            ignore: ${{ .hidden }}
          _sch__dd_nodes_e__tag_existing:
            label: Number of nodes
            type: number
            default: 1
            hidden: true
            ignore: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' != inputs.pwrl_host.resource.provider }}
          _sch__dd_partition_e__tag_cloud:
            type: slurm-partitions
            label: SLURM partition
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  || 'existing' == inputs.pwrl_host.resource.provider }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip: >-
              Partition to submit the interactive job. Leave empty to let SLURM
              pick the optimal option.
            resource: ${{ inputs.pwrl_host.resource }}
          _sch__dd_time_e_:
            label: Walltime
            type: string
            default: 01:00:00
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype  }}
            ignore: ${{ .hidden }}
            tooltip: e.g. 01:00:00 - Amount of time slurm will honor the interactive session.
          scheduler_directives_slurm:
            type: string
            label: Scheduler directives
            hidden: ${{ 'SLURM' != inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip: >-
              e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ;
              to separate parameters. Do not include the SBATCH keyword.
          _sch__d_q___:
            type: string
            label: PBS queue
            hidden: ${{ 'PBS' != inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: ${{ .hidden }}
            tooltip:
              Queue to submit the interactive job. Must select one! Use [qstat -f
              -Q] to list all queues on the system
          scheduler_directives_pbs:
            type: string
            label: Scheduler directives
            hidden: ${{ 'PBS' != inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip:
              e.g. -l mem=1000;-l nodes=1:ppn=4 - Use the semicolon character ; to
              separate parameters. Do not include the PBS keyword.
        collapsed: false
      service:
        type: group
        label: Jupyter Lab Settings
        items:
          name:
            type: string
            hidden: true
            default: jupyterlab-host
          notebook_dir:
            label: Directory to start Jupyter Lab GUI
            type: string
            default: __HOME__
            tooltip: This is the directory that you start with when the Jupyter graphical user interface starts. The default value here is your home directory.
          rootless_docker:
            label: Use Rootless Docker?
            type: boolean
            default: true
            hidden: true
          conda_install:
            label: Install miniconda environment if not there?
            type: boolean
            default: true
            tooltip: Select Yes to install Jupyter in miniconda environment and No to load an existing python environment
          parent_install_dir:
            label: Parent Install Directory
            type: string
            default: __HOME__/pw/software
            hidden: ${{ inputs.service.conda_install == false }}
            ignore: ${{ .hidden }}
            tooltip: Software dependencies are installed in this directory. Ensure the directory has sufficient space!
          conda_install_dir:
            label: Name of the Conda Installation Directory
            type: string
            default: .miniconda3c
            hidden: ${{ inputs.service.conda_install == false }}
            ignore: ${{ .hidden }}
            tooltip: Ensure the directory has sufficient space for Conda and its packages.
          conda_env:
            label: Conda environment
            type: string
            default: base
            hidden: '${{ inputs.service.conda_install == false }}'
            optional: ${{ .hidden }}
            ignore: ${{ .hidden }}
            tooltip: Environment to active. The base environment enables changing kernel to other environments!
          load_env:
            label: Command to load Jupyter Notebook to the PATH
            type: string
            default: source __HOME__/pw/.miniconda3c/etc/profile.d/conda.sh; conda activate base
            hidden: ${{ inputs.service.conda_install == true }}
            optional: ${{ .hidden }}
            ignore: ${{ .hidden }}
            tooltip: Use a bash command
          password:
            label: Password for notebook session
            type: password
            optional: true
            hidden: true
            ignore: true
            tooltip: Enter password or leave blank for no password
          install_instructions:
            label: Select Jupyter Lab Installation
            type: dropdown
            hidden: ${{ inputs.service.conda_install == false }}
            ignore: ${{ .hidden }}
            default: jupyterlab4.1.5-python3.11.5
            options:
              - value: jupyterlab4.1.5-python3.11.5
                label: Jupyter Lab 4.1.5 with Python 3.11.5
              - value: latest
                label: Latest versions of Jupyter Lab and Python (not thoroughly tested)
              - value: dask-extension-jupyterlab
                label: Dask dependencies for PW
              - value: yaml
                label: Provide custom Conda environment YAML file
          yaml:
            label: Paste Conda Environment Defition YAML
            type: editor
            hidden: ${{ inputs.service.install_instructions != yaml ||  inputs.service.conda_install == false }}
            optional: ${{ .hidden }}
            ignore: ${{ .hidden }}
            tooltip: Copy and paste a custom Conda environment definition YAML file
          install_kernels:
            label: Select additional kernels to install
            type: multi-dropdown
            optional: true
            hidden: ${{ inputs.service.conda_install == false }}
            ignore: ${{ .hidden }}
            options:
              - value: julia-kernel
                label: Julia Kernel
              - value: R-kernel
                label: R Kernel
